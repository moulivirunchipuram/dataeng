{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade setuptools\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install python-snappy\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "print('Creating spark session on AWS')\n",
    "spark = create_spark_session()\n",
    "\n",
    "\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "song_input_data = \"data/songdata/song_data/A/A/A/*.json\"\n",
    "log_input_data = \"data/logdata/*.json\"\n",
    "output_data = \"data/outputdata/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Read song data from json file')\n",
    "song_data = spark.read.json(song_input_data)\n",
    "    \n",
    "# read song data file\n",
    "print('Print song data schema')\n",
    "df = song_data\n",
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Extract columns to create song table')\n",
    "   \n",
    "#print('Songs table: ')\n",
    "songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\")\n",
    "print(songs_table.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_songs_table = songs_table.toPandas()\n",
    "year_list = list(set(df_songs_table['year'].tolist()))\n",
    "type(year_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_id_list = list(set(df_songs_table['artist_id'].tolist()))\n",
    "type(artist_id_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_songs_table.loc[(df_songs_table['year']==int(1982)) & (df_songs_table['artist_id']==str('AR7G5I41187FB4CE6C'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "print('Writing to parquet')\n",
    "songs_table.write.partitionBy('year', 'artist_id').parquet(os.path.join(output_data, 'songs/songs.parquet'), 'overwrite')\n",
    "print(\"done songs_table write\")\n",
    "#for year in year_list:\n",
    "#    for artist_id in artist_id_list:\n",
    "#        df_to_parquet = df_songs_table.loc[(df_songs_table['year']==int(year)) & (df_songs_table['artist_id']==str(artist_id))]\n",
    "#        df_to_parquet.to_parquet(\"{}/songs_table/{}/{}/songs_table.parquet\".format(output_data,year,artist_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Artist table: ')\n",
    "artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\")\n",
    "print(artists_table.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Process Log output data\n",
    "# get filepath to log data file\n",
    "log_data = spark.read.json(log_input_data)\n",
    "\n",
    "# read log data file\n",
    "print('Print song data schema')\n",
    "log_df = log_data\n",
    "print(df.count())\n",
    "log_df.printSchema()\n",
    "print(log_df.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Users table: ')\n",
    "users_table = log_df.select(\"firstName\", \"lastName\", \"gender\", \"level\", \"userId\")\n",
    "print(users_table.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "log_df = log_df.withColumn(\"timestamp\", get_timestamp(log_df.ts))\n",
    "log_df.printSchema()\n",
    "log_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: F.to_date(x), TimestampType())\n",
    "log_df = log_df.withColumn(\"start_time\", get_timestamp(log_df.ts))\n",
    "log_df.printSchema()\n",
    "log_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "log_df = log_df.withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"day\", F.dayofweek(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"week\", F.weekofyear(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"month\", F.month(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"year\", F.year(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"weekday\", F.dayofweek(\"timestamp\"))\n",
    "#log_df.printSchema()\n",
    "print(log_df.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table = log_df.select(\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "time_table.createOrReplaceTempView(\"time_table\")\n",
    "print(\"Created temp view time_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create sql query to partition on year and month\n",
    "time_table = spark.sql(\n",
    "    \"\"\"SELECT DISTINCT start_time, hour, day, week, month, year, weekday\n",
    "    FROM time_table \n",
    "    ORDER BY year, month\n",
    "    \"\"\")\n",
    "# write time table to parquet files partitioned by year and month\n",
    "print('Writing time_table to parquet')\n",
    "df_time_table = time_table.toPandas()\n",
    "year_list = list(set(df_time_table['year'].tolist()))\n",
    "month_list = list(set(df_time_table['month'].tolist()))\n",
    "\n",
    "time_table.write.partitionBy('year', 'month').parquet(os.path.join(output_data, 'timetable/time_table.parquet'),\n",
    "                                                           'overwrite')\n",
    "\n",
    "year_list = list(set(df_time_table['year'].tolist()))\n",
    "month_list = list(set(df_time_table['month'].tolist()))\n",
    "time_table.write.partitionBy('year', 'month').parquet(os.path.join(output_data, 'timetable/time_table.parquet'),\n",
    "                                                           'overwrite')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.json(song_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_df.createOrReplaceTempView(\"log_df_table\")\n",
    "song_df.createOrReplaceTempView(\"song_df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "# songplay_id, \n",
    "# start_time, \n",
    "# user_id, \n",
    "# level, \n",
    "# song_id, \n",
    "# artist_id, \n",
    "# session_id, \n",
    "# location, \n",
    "# user_agent\n",
    "\n",
    "songplays_table = spark.sql(\n",
    "    \"\"\"SELECT log_df_table.start_time, log_df_table.userId, log_df_table.level, log_df_table.sessionId, log_df_table.location, log_df_table.userAgent, song_df_table.song_id, song_df_table.artist_id \n",
    "    FROM log_df_table \n",
    "    INNER JOIN song_df_table \n",
    "    ON song_df_table.artist_name = log_df_table.artist \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table = songplays_table.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "print('Writing to parquet')\n",
    "songplays_table.write.parquet(\"data/songplays_table.parquet\",'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
